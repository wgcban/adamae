# *Ada*MAE: Adaptive Masking for Efficient Spatiotemporal Learning with Masked Autoencoders

- We propose *Ada*MAE, a novel, adaptive, and end-to-end trainable token sampling strategy for MAEs that takes into account the spatiotemporal properties of all input tokens to sample fewer but informative tokens.

- We empirically show that \adamae samples more tokens from high spatiotemporal information regions of the input, resulting in learning meaningful representations for downstream tasks.

![intro-fig](figs/adamae-intro-fig.jpeg)

## Comparision of adaptive masking with existing masking techniques

![intro-fig](adamae/figs/adamae-mask-types.jpeg)
